# git_bug

Name the most common Input Formats defined in Hadoop? Which one is default?
Answer:
The two most common Input Formats defined in Hadoop are:

TextInputFormat
KeyValueInputF5ormat
SequenceFileInputFormat
TextInputFormat is the Hadoop default.

Question: 2
What is InputSplit in Hadoop?
Answer:
When a Hadoop job is run, it splits input files into chunks and assign each split to a mapper to process. This is called InputSplit.

Question: 3
What platform and Java version is required to run Hadoop?
Answer:
Java 1.6.x or higher version are good for Hadoop, preferably from Sun. Linux and Windows are the supported operating system for Hadoop, but BSD, Mac OS/X and Solaris are more famous to work.

Question: 4
What kind of Hardware is best for Hadoop?
Answer:
Hadoop can run on a dual processor/ dual core machines with 4-8 GB RAM using ECC memory. It depends on the workflow needs.

Question: 5
How is the splitting of file invoked in Hadoop framework?
Answer:
It is invoked by the Hadoop framework by running getInputSplit()method of the Input format class (like FileInputFormat) defined by the user.
What is the purpose of RecordReader in Hadoop?
Answer:
The InputSplit has defined a slice of work, but does not describe how to access it. The RecordReader class actually loads the data from its source and converts it into (key, value) pairs suitable for reading by the Mapper. The RecordReader instance is defined by the Input Format.

Question: 7
What is InputSplit in Hadoop? Explain.
Answer:
When a hadoop job runs, it splits input files into chunks and assign each split to a mapper for processing. It is called InputSplit.

Question: 8
What is a Combiner?
Answer:
The Combiner is a ‘mini-reduce’ process which operates only on data generated by a mapper. The Combiner will receive as input all data emitted by the Mapper instances on a given node. The output from the Combiner is then sent to the Reducers, instead of the output from the Mappers.

Question: 9
How many InputSplits is made by a Hadoop Framework?
Answer:
Hadoop will make 5 splits as following:

One split for 64K files
Two splits for 65MB files, and
Two splits for 127MB files
What is JobTracker?
Answer:
JobTracker is the service within Hadoop that runs MapReduce jobs on the cluster.
What are some typical functions of Job Tracker?
Answer:
The following are some typical tasks of JobTracker:-

Accepts jobs from clients
It talks to the NameNode to determine the location of the data.
It locates TaskTracker nodes with available slots at or near the data.
It submits the work to the chosen TaskTracker nodes and monitors progress of each task by receiving heartbeat signals from Task tracker.
Question: 12
What is Map/Reduce job in Hadoop?
Answer:
Map/Reduce is programming paradigm which is used to allow massive scalability across the thousands of server.
Actually MapReduce refers two different and distinct tasks that Hadoop performs. In the first step maps jobs which takes the set of data and converts it into another set of data and in the second step, Reduce job. It takes the output from the map as input and compress those data tuples into smaller set of tuples.

Question: 13
Define TaskTracker.
Answer:
TaskTracker is a node in the cluster that accepts tasks like MapReduce and Shuffle operations from a JobTracker.

Question: 14
What is Hadoop Streaming?
Answer:
Hadoop streaming is a utility which allows you to create and run map/reduce job. It is a generic API that allows programs written in any languages to be used as Hadoop mapper.

Question: 15
What is a combiner in Hadoop?
Answer:
A Combiner is a mini-reduce process which operates only on data generated by a Mapper. When Mapper emits the data, combiner receives it as input and sends the output to reducer.
Is it necessary to know java to learn Hadoop?
Answer:
If you have a background in any programming language like C, C++, PHP, Python, Java etc. It may be really helpful, but if you are nil in java, it is necessary to learn Java and also get the basic knowledge of SQL.

Question: 17
How to debug Hadoop code?
Answer:
There are many ways to debug Hadoop codes but the most popular methods are:

By using Counters.
By web interface provided by Hadoop framework.
Question: 18
What is distributed cache in Hadoop?
Answer:
Distributed cache is a facility provided by MapReduce Framework. It is provided to cache files (text, archives etc.) at the time of execution of the job. The Framework copies the necessary files to the slave node before the execution of any task at that node.






-----------------------------------------




1. What kind of issues your facing while using cluster


Answer:
when we are not using any particular vendor like Cloudera, MapR, Hortonworks
we may face this issue (also leads in performance degradation) read about performance tuning here


Lack of configuration management
Poor allocation of resources
Lack of a dedicated network
Lack of monitoring and metrics
Ignorance of what log files contain what information
Drastic measures to address simple problems
Inadvertent introduction of single points of failure
Over reliance on defaults
Cluster issues are somehow related to Admin team.other task that need to be manage daily are
Managing space between application users
Distcp - Data back ups and migration
Managing Services and adding nodes using Ambari 
Changing cluster capacity 
user/group permission management
Alerts and Notifications
Script configuration
Interfaces setup

2. Please mention recommend hard-disk and ram size .

This is something related to cluster management .
Development side PC requirement: Commodity hardware

Definition: Commodity hardware is a non-expensive system which is not of high quality or high-availability. Hadoop can be installed in any average commodity hardware. We don’t need super computers or high-end hardware to work on Hadoop

We need to care about daily data you are processing and how long you are storing it into local system.

Normally PC with 8 GB Ram and 1TB hard disk can be preferred

Cluster Management: Production Environment
Namenode and Secondary name node should have high capacity.
We have to plan about: storage , CPU, memory, network bandwidth?

3. Hadoop 1 or Hadoop 2 Which one you are using.

Answer version which you are using.

Hadoop 2.x is preferred because of YARN. Resource allocation, High Availability and Federation

4. Are you using any Hadoop distribution.
Cloudera offers CDH (Cloudera's Distribution including Apache Hadoop) and Cloudera Enterprise. 
Hortonworks (formed by Yahoo and Benchmark Capital) Hortonworks provides Hortonworks Data Platform (HDP). 
MapR – the MapR Distribution for Apache Hadoop
More detail can be fetched from here.
Considerations should be taken into account when choosing a Hadoop Distribution can be check at here

5.Have you used oozie and zookeeper in cluster

Zookeeper : The Zookeeper is used for managing ‘coordination related’ data. It has simple Client Server model architecture

Oozie –


Used for Hadoop job scheduler.
also used for Monitoring and Alerting Hadoop jobs 

Types of Oozie Jobs
Periodical/Coordinator Job
Oozie Hadoop Workflow
Oozie Bundle

6. If you used what kind of jobs can you explain me

Mostly we use it to schedule job at cluster node instead of running manual script each time.
Alert mails are triggered when threshold value is reached.

7. What trouble shooting issues you faced 
Issues can be related to cluster or logs like 
IO exception error 
cluster in safe mode
host unreachable,
Change in host identification
sderr in logs

I am explaining a simple issue
            Error while running a example more than once. Output file already exist. 
Solution:
use following command to check output exist or not
bin/hadoop jar hadoop-*-examples.jar \
grep input output2 'dfs[a-z.]+'
                       or 
bin/hadoop dfs -rmr output can be used.

Improper syntax also cause some trouble shooting issue


8. Cluster maintenance and backup

FileSystem Checksrecursively Health check up
 sudo -u hdfs hadoop fsck /
HDFS Balancer utility
sudo -u hdfs hdfs balancer -threshold <threshold-value>
Adding or Decommissioning nodes to the cluster
Node Failures
Database and Metadata Backups for individual database dumps.
Purging older log files
Plan unplaned downtime
Network issue (host unreachable)


9. Have you use any monitoring tools have like gangila.
Ganglia and Nagios.

Ganglia is a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids. We haven’t used
Ganglia is more concerned with gathering metrics and tracking them over time while Nagios has focused on being an alerting mechanism.

10. What is the roles and responsibilities of your project?
Hadoop Profiles

Developer
Architect
Admin
Tester
Hadoop Support
Data Scientist


Job Responsibilities of a Hadoop Developer:
A Hadoop Developer has many responsibilities. 

loading data from RDBMS to HDFS or Hbase using Sqoop
Bulk or Batch data processing.
Write components to integrate/automate the system in MapR, 
Data Analysis
Hive , Pig Query
Using Streaming API
In short we have to explain about
 where you will get data, data loading, data analysis and what is final business use case of your project





 Share this PostFacebook shareTwitter sharePin ThisShare on TumblrShare on Google PlusEmail This
4 comments:   
Labels: Hadoop, hadoop admin real time interview questions, hadoop real time interview questions, real time hadoop interview question
 
 
Older Posts Home
Subscribe to: Posts (Atom)

BLOG ARCHIVE
▼  2016 (50)
▼  September (2)
Answer to Hadoop Real time Questions
Difference between Identification, authentication,...
►  August (10)
►  June (10)
►  May (27)
►  March (1)
►  2015 (6)
MY BLOG ROLL
UNREVEALED UNSPOKEN
FOLLOWERS

Buy Anything from Amazon at discount rates 
SEARCH THIS BLOG

Search
For learning Hadoop you can go
through best online trainer

***UDEMY****
Udemy Generic 125x125
If you are a Blogger ad want to earn some extra cash
Register at this Affiliate Program
and Double your Earning

Rakuten Affiliate Network Welcome Program
Enter your email address:



Subscribe
Delivered by FeedBurner

POPULAR POSTS
Hadoop Useful Commands
Hadoop Namenode Commands
Top Hadoop Interview Questions
LABELS
Hadoop (31) Hadoop MCQ (12) Hadoop Quiz (11) Interview Question (9) Download (3) best book (3) books on hadoop (3) Test (2) hadoop in action free download (2) hadoop in action pdf (2) 4th edition (1) Hive (1) Joining Multiple Tables in Single query (1) Set 1 (1) Set 2 (1) Set 3 (1) Set 4 (1) apache hadoop in action (1) apache hadoop in practice (1) hadoop for dummies (1) hadoop in action ebook download (1) hadoop in practice (1) hadoop in practice ebook download (1) hadoop in practice pdf (1) hadoop the definitive guide (1)
WRITE US YOUR QUERY
Name 

Email * 

Message * 

 Send
Picture Window template. Powered by Blogger.
ShareThis Copy and Paste - See more at: https://hadoopquiz.blogspot.in/#sthash.4qxOcwLO.dpuf - See more at: https://hadoopquiz.blogspot.in/#sthash.4qxOcwLO.dpuf

